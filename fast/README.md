# âš¡ fast Module

This directory contains all code and resources related to the custom tokenizer and action sequence compression for the FAST method.

---

## Overview

The `fast` module provides tools for:
- Training a custom BPE-like tokenizer for quantized action sequences.
- Encoding and decoding action sequences using the trained tokenizer.
- Managing vocabulary and merge rules for efficient action representation.

---

## Main Files & Their Functions

- **train_tokenizer.py**  
  - Purpose: Trains a custom tokenizer using a BPE-like algorithm on quantized action sequence corpus.
  - Key Steps:
    - Loads the base vocabulary and corpus.
    - Iteratively merges the most frequent adjacent token pairs until the target vocabulary size is reached.
    - Saves the resulting vocabulary and merge rules to `tokenizer/fast_tokenizer.json`.
  - Usage:
    ```bash
    python -m fast.train_tokenizer
    ```

- **encoder.py**  
  - Purpose: Encodes a space-separated action sequence string into a list of tokens and corresponding token IDs using the trained tokenizer.
  - Key Function:
    - `encoder(sequence, tokenizer_path, unk_token="[UNK]")`: Returns (tokens, ids) for a given sequence.

- **decoder.py**  
  - Purpose: Decodes a list of token IDs back into the original quantized action sequence (as integer list).
  - Key Function:
    - `decoder(ids, tokenizer_path, skip_special_tokens=False)`: Returns the decoded integer sequence.

---

## Tokenizer Resources

- **tokenizer/fast_tokenizer_base_vocab.txt**  
  - The base vocabulary generated by utils/build_corpus.py for the tokenizer, including special tokens and all possible quantized action values.

- **tokenizer/fast_tokenizer_corpus.txt**  
  - The training corpus generated by utils/build_corpus.py for the tokenizer, each line is a quantized action sequence (space-separated).

- **tokenizer/fast_tokenizer.json**  
  - The trained tokenizer configuration, including the final vocabulary and merge rules.

---

## Typical Workflow

1. **Prepare Corpus and Base Vocabulary**  
   Use utils/build_corpus.py to generate and save to `tokenizer/fast_tokenizer_corpus.txt` and `tokenizer/fast_tokenizer_base_vocab.txt`.

2. **Train Tokenizer**  
   Run `train_tokenizer.py` to build a compact vocabulary and merge rules:
   ```bash
   python -m fast.train_tokenizer